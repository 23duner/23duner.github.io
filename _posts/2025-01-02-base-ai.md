---
title: 人工智能基础扫盲
layout: post
post-image: "https://raw.githubusercontent.com/thedevslot/WhatATheme/master/assets/images/SamplePost.png?token=AHMQUEPC4IFADOF5VG4QVN26Z64GG"
description: A sample post to show how the content will look and how will different
  headlines, quotes and codes will be represented.
tags:
- 面试题
- 人工智能八股
- 寒假实习
---

This post will show you how the content will look like in the post pages and how the headlines, quotes and quotes will be represented. Jekyll is mainly used to write simple markdown and after that it renders out a static pages, so you need to know the basics of writing markdown for that.
For more information about writing markdown you can checkout the following markdown cheatsheets:
* [Mastering Markdown](https://guides.github.com/features/mastering-markdown/)
* [Markdown Guide](https://www.markdownguide.org/cheat-sheet/)
* [GitHub Flavored Markdown Spec](https://github.github.com/gfm/)

---
# 初心

我就知道你什么都不会

但速成怎么不能学习了

围绕简历准备基础知识 算法怎么办但是

机会留给有准备的人和能把握住机会的人

## 整理一下学习资料

刷题 nb

https://www.julyedu.com/questions/interview-list?kp\_id=30\&subject=NLP

datawhale为什么是神

https://www.datawhale.cn/learn?learnPage=1

https://www.datawhale.cn/learn/summary/40

很牛的面经 有空把里面的知识点过一遍

https://zhuanlan.zhihu.com/p/426349482

# 名词解释 当问到问答的时候应该怎么做

1. 神经网络

   1. 前馈神经网络  信号在前向传播过程中不会回溯  涉及反向传播 (Backpropagation) 算法

   2. 人工神经网络ANN&#x20;

   3. RNN循环神经网络 收到短时记忆的影响

   4. 误差反向传播  计算输出层的误差 反向传播到隐藏层 计算每一层的误差计算梯度（即损失函数相对于权重的导数） 使用梯度下降算法更新权重

   5. 需要大量标注数据

2. 激活函数

   1. 神经网络的基本组成单元是线性操作 比如矩阵乘法加法

   2. 主要功能是引入非线性，使神经网络能够**学习和表示复杂的模式和关系**

   3. 根据选择特征函数的不同 可以对输入数据进行特征映射 筛选 选择

   4. 限制输出

   ```plain&#x20;text
   ReLU（Rectified Linear Unit）：一种简单的激活函数，可以解决梯度消失问题，加快训练速度。f(x)=max(0,x)
   可导性分析：
   ```

![](https://gvl632fo2qu.feishu.cn/space/api/box/stream/download/asynccode/?code=OTg5NDJmYmY2OWJlZjRmYzMxNGFmMTg0ZWUyNjQ3OTNfWnlxQzZER093dlU3UlZBVVNRS0I3VGlOQ0kzZXlGb01fVG9rZW46SU9SMmJzcUFUb0NQQll4OXJsUGNSVGdubjQ4XzE3MzczNjA3MjI6MTczNzM2NDMyMl9WNA)

其实也就是对应参数在那里不会被更新

```latex
GeLU（Gaussian Error Linear Unit）：一种改进的ReLU函数，可以提供更好的性能和泛化能力   其实就是更加平滑的relu吧
Swish：一种自门控激活函数，可以提供非线性变换 f(x)=x*sigmoid(x)
Tanh:帮助调节流经神经网络的值，始终限制在-1到1
```

* 反向传播

  1. 链式法则

  2. 通过前向传播得到结果后与标签比较会出现误差，再通过反向传播把误差更新到每一层去

* 梯度消失

梯度是用来通过反向传播算法更新损失函数在每个参数方向上的变化速度

深度神经网络的反向传播过程中，梯度变得非常小

* 梯度爆炸



* softmax函数   就是soft版本的max？？？？？

  1. 把数值映射为概率分布 还能增加类别之间的差异性 通过指数运算让大的更大 小的更小

$$p_i=\frac{exp(z_i)}{\sum^n_{j=1}exp(z_j)}$$

* 英语

  1. In-effeciency 低效

* 序列性数据：

尤其是对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣（比如机器翻译中，有可能只把每个词都翻译出来了，但是不能组织成合理的句子）。

![](https://gvl632fo2qu.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2NiZDQ5Mzg3MWQ5YTA4NjU4ZTY3M2ZhMjlhZTg1MGRfNGZxU05UR2R4R3Bzb0tvcUFqM3NrRlMyWFVVRlhtSlNfVG9rZW46TDdCSmJ1YW9ab3hFdHl4YUR4R2NzY3hXbkxmXzE3MzczNjA3MjI6MTczNzM2NDMyMl9WNA)



* 非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素

* 梯度弥散就是梯度消失

* 什么是卷积

对图像（不同的数据窗口数据）和滤波矩阵（又称卷积核（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。

* 什么是池化

  1. 最大池化

  2. 平均池化

  3. 减少数据量 减少数据维数

  4. 防止过拟合 防止对噪声过度学习

  5. 提取主要特征  比如在人脸识别中，池化操作可以帮助提取面部的关键轮廓特征。

  6. 深度学习中聚合操作

* 下采样

  1. 数据处理技术 新数据在样本数量或维度上低于原始数据

* 参数更新方法

# Llm

在自然语言处理（NLP）领域，tokens 通常指的是文本经过某种方式分割后得到的基本单元

## 微调&#x20;

我服

重生之我在公司学微调 其实有点难以置信

https://blog.csdn.net/qq\_39172059/article/details/136693607

https://blog.csdn.net/qq\_45591302/article/details/140150276

### sft监督微调

### Prompt tuning

只调整prompt tokens

Prompt Tuning 是通过在输入序列前添加额外的 Token 来适配下游任务的方法。这些额外的 Token 是可训练的，而预训练语言模型的参数保持不变。  其实就是加了好多\[mask]

### P-tuning

### Lora

Low-Rank adaptation

俩低秩矩阵 先降维 再升维

### Q\_lora

量化加lora



指令数据&#x20;

### RLHF

基于人类反馈对语言模型进行强化学习

Reinforcement learning from human feedback



## 评估

https://blog.csdn.net/wshzd/article/details/135689899

年多前，随着Stable Diffusion和ChatGPT的发布，生成式人工智能成为主流，几乎每周都会有新的模型发布

1. 自然语言处理中古老的指标：文本摘要或机器翻译  现有的基于规则的文本生成指标 (如 BLUE 和 ROUGE)

2. 研究基准：编程运行大量问题和答案

3. llm自我评估

4. 人工评价





## RAG

（Retrieval - Augmented Generation）即检索增强生成技术，是一种结合信息检索和语言生成的技术。它的核心在于利用外部知识源来增强语言模型的生成能力，使模型能够生成更准确、更有依据的回答。

* **检索部分**：当接收到一个问题或任务时，首先在外部知识源（如文档数据库、知识图谱等）中进行检索。这个检索过程可以基于各种检索算法，例如向量空间模型、倒排索引等。检索的目的是找到与问题相关的知识片段，这些知识片段可以是文本段落、知识图谱中的实体关系等。

* **生成部分**：将检索到的知识片段作为额外的输入提供给语言生成模型（如 Transformer 架构的语言模型）。语言模型利用这些知识以及自身学习到的语言模式来生成回答。例如，在生成自然语言回答时，语言模型会根据问题的语义、检索到的知识和自身训练的语言规则来构建句子结构，选择合适的词汇，从而生成一个完整的回答。

应用: 问答系统，内容生成，知识图谱填充和生成

## re-ranking

### langchain中应用RAG

1. 对于给定的输入（问题），模型首先使用检索系统从大型文档集合中查找相关的文档或段落。这个检索系统通常基于密集向量搜索，例如ChromaDB、Faiss这样的向量数据库。

2. **上下文编码**：找到相关的文档或段落后，模型将它们与原始输入（问题）一起编码。

3. **生成**：使用编码的上下文信息，模型生成输出（答案）。这通常当然是通过大模型完成的。

RAG 的一个关键特点是，它不仅仅依赖于训练数据中的信息，还可以从大型外部知识库中检索信息。这使得RAG模型特别适合处理在训练数据中未出现的问题。

langchain做到了什么

langchain都提供了相关工具

1. 数据准备与加载&#x20;

2. 数据预处理

3. embedding嵌入向量生成

   1. 将文本转换为向量表示 这些向量将用于后续的相似度检索

4. 向量存储与检索

5. 语言模型生成回答

   1. 构建提示



放到一起对比 果然人只能答出来真正理解的东西吗 也不一定

lstm和transformer区别

对agent的理解 多智能体协同

## Agent

在计算机科学和人工智能领域，agent（智能体）是一种能够感知环境、进行决策并采取行动以实现特定目标的实体。它可以是软件程序、机器人或其他具有自主行为能力的系统。

你觉得如果你能回答上来&#x20;

感知 决策 行动&#x20;

会不会很好

多智能体协同的斯坦福小镇

Agent 你写过的论文 拿出来吓唬人

## nlp神经网络

&#x20;NLP 问题开始引入神经网络模型的时期。使用最广泛的三种主要的神经网络是：循环神经网络、卷积神经网络和递归神经网络。

### RNN\&LSTM

1. woc我好像懂了点什么

   1. RNN

   $$y_t=f(y_{t-1},x_t)$$
   逐步递归才能获得全局信息，所以一般用双向RNN

   * CNN卷积神经网络

$$y_t=f(x_{t-1},x_t,x_{t+1})$$
以尺寸为三的卷积为例



* 梯度

  1. 对于rnn它是一个链式乘积。如果$$ ∂ht∂ht−1\frac{\partial h_t}{\partial h_{t-1}}∂ht−1∂ht$$ 的范数小于 1，梯度会指数级衰减（**梯度消失**）；如果大于 1，梯度会指数级增长（**梯度爆炸**）

  2. 因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸

### LSTM

1. 是RNN（是神经网络）的变体啊 人家叫长短期记忆神经网络

2. LSTM 的核心创新是引入了 **门机制（gates）** 和 **单元（细胞）状态（cell state）**，为信息流动增加了控制力，能够有效缓解梯度问题。细胞状态用于长期记忆信息，门控机制则控制信息的流动。

简单推导：sigmoid层有分类作用 保留或舍弃

sigmoid 用在了各种gate上，产生0\~1之间的值，这个一般只有sigmoid最直接了，相当于要么是1则记住，要么是0则忘掉。

tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。

### Attention

这里是和全连接层区分开的

什么是注意力机制？

这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段

[《Attention is All You Need》浅读(简介+代码) - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/4765)

![](https://gvl632fo2qu.feishu.cn/space/api/box/stream/download/asynccode/?code=OGIyNGE2MzgxYTIzNmM0MjZhNzM3MmI2NmUyYzNiOWFfQUJVSmxJTVh4R1IwSzJOUUQyZnk1aUk3OTFJNkpBRFZfVG9rZW46SU9PMGJBR0Z3b2tURGx4eTJyeGN1d3FBblpjXzE3MzczNjA3MjI6MTczNzM2NDMyMl9WNA)

1. Scaled-Dot Attention缩放点积注意力

**Query**: 当前需要关注的内容。

**Key**: 每个输入的特征。

**Value**: 实际包含的信息内容。

### Transformer

* 在处理一个序列时，

  1. 位置编码：Transformer 首先将输入序列（如文本经过词向量表示后的序列）与位置编码相加，

  2. 多头注意力：然后将其送入多头注意力机制进行处理。

  3. 前馈神经网络：经过多头注意力机制后的输出再通过前馈神经网络进行进一步的变换。

* 在 Transformer 的编码器（Encoder）部分，这种结构会被重复多次（通常是多层堆叠），每一层都能够学习到更复杂的语义表示。在 Transformer 的解码器（Decoder）部分，结构类似，但还包含了一些额外的机制，如在多头注意力机制中会有一个掩码（Mask）来防止解码器在生成过程中看到未来的信息，以保证生成过程的顺序性。

1. 多头注意力机制&#x20;

很多时候不需要从原理角度研究问题吧&#x20;

* 位置编码

* 前馈神经网络



* Transformer：具有很强的并行计算能力。因为其多头注意力机制和前馈神经网络部分都是可以并行计算的，在处理序列数据时，不需要像 LSTM 那样顺序地处理每个时间步。这使得 Transformer 在训练和推理过程中能够更高效地利用计算资源，尤其是在处理长序列时，能够显著缩短计算时间。

* LSTM：由于其循环结构，在每个时间步的计算都依赖于上一个时间步的隐藏状态，所以很难进行并行计算。在处理长序列时，计算效率相对较低，因为必须按照顺序依次处理每个时间步。



# 深度学习

深度学习主要还是神经网络 不要担心

## ResNet

Residual neural network

残差神经网络



## CNN

CNN 即卷积神经网络（Convolutional Neural Network），

具有网格结构数据（如图像、音频）

1. 卷积层  通过卷积核在输入数据上滑动 可以提取输入数据的局部特征

2. 池化层（更广泛的叫法是下采样层

   1. 保留主要的特征同时减少参数和计算量，降低数据维度，过滤噪声防止过拟合，提高模型泛化能力；

3. 全连接层

## GAN

生成对抗网络 生成器 判别器

# CV

# 机器学习

你以为这个你就会吗

## 指标

召回率准确率各种指标

1. 准确率 分类正确的样本数占总样本数的比例。它是最直观的分类性能衡量指标

   1. 真正例+真负例/真正例+真负例+假负例+假正例

2. 精确率 精确率是指在被模型预测为正类的样本中，真正为正类的样本所占的比例

   1. 精确到正类里面研究

3. ROC曲线

4. **分类任务指标**



&#x20;   \- **准确率（Accuracy）**

&#x20;       \- \*\*定义\*\*：准确率是指分类正确的样本数占总样本数的比例。它是最直观的分类性能衡量指标。公式为：$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$，其中 $TP$（True Positive）是真正例，即实际为正类且被模型预测为正类的样本数；$TN$（True Negative）是真负例，即实际为负类且被模型预测为负类的样本数；$FP$（False Positive）是假正例，实际为负类但被预测为正类的样本数；$FN$（False Negative）是假负例，实际为正类但被预测为负类的样本数。

&#x20;       \- \*\*示例\*\*：在一个猫狗图像分类任务中，总共有100张图像，模型正确分类了80张（其中狗正确分类30张，猫正确分类50张），那么准确率就是 $80\div100 = 0.8$ 或 $80\\%$。

&#x20;       \- \*\*适用场景\*\*：适用于各类分类问题，尤其是当数据集中不同类别的样本分布相对均衡时，准确率能够很好地反映模型性能。

&#x20;       \- \*\*局限性\*\*：当数据类别不平衡时，准确率可能会产生误导。例如，在一个疾病诊断任务中，只有 $1\%$ 的患者患病（正类），如果模型总是预测为健康（负类），准确率仍可能很高，但实际上模型没有任何诊断价值。



&#x20;   \- **精确率（Precision）**

&#x20;       \- \*\*定义\*\*：精确率是指在被模型预测为正类的样本中，真正为正类的样本所占的比例。公式为：$Precision = \frac{TP}{TP + FP}$。

&#x20;       \- \*\*示例\*\*：在垃圾邮件分类中，模型预测有50封邮件是垃圾邮件（正类），其中真正是垃圾邮件的有40封，那么精确率为 $40\div50 = 0.8$。

&#x20;       \- \*\*适用场景\*\*：当重点关注模型预测为正类的准确性时使用，比如在推荐系统中，推荐给用户的商品（预测为正类）是否真正是用户感兴趣的（真正正类）很重要，此时精确率是一个关键指标。

&#x20;       \- \*\*局限性\*\*：单独使用精确率可能无法全面评估模型，因为它只考虑了预测为正类的情况，没有考虑实际正类被遗漏的情况。



&#x20;   \- **召回率（Recall）**

&#x20;       \- \*\*定义\*\*：召回率是指在实际为正类的样本中，被模型正确预测为正类的样本所占的比例。公式为：$Recall = \frac{TP}{TP + FN}$。

&#x20;       \- \*\*示例\*\*：在一个寻找失联人员（正类）的任务中，总共有10个失联人员，模型成功找到7个，那么召回率为 $7\div10 = 0.7$。

&#x20;       \- \*\*适用场景\*\*：当重点关注正类样本是否被全部找出时使用，例如在癌症筛查中，希望尽可能找出所有患癌的病人，召回率就显得尤为重要。

&#x20;       \- \*\*局限性\*\*：只关注召回率可能会导致模型为了找出更多正类而过度预测，使得精确率下降。



&#x20;   \- **F1 - score**

&#x20;       \- \*\*定义\*\*：F1 - score是精确率和召回率的调和平均数，它综合考虑了精确率和召回率。公式为：$F1 = 2\times\frac{Precision\times Recall}{Precision + Recall}$。

&#x20;       \- \*\*示例\*\*：若精确率为0.8，召回率为0.7，那么 $F1 - score = 2\times\frac{0.8\times0.7}{0.8 + 0.7}\approx0.747$。

&#x20;       \- \*\*适用场景\*\*：当需要在精确率和召回率之间进行平衡时使用，例如在信息检索中，既希望检索出的结果尽可能准确（精确率），又希望能把所有相关信息都检索出来（召回率），F1 - score可以很好地衡量模型在这两方面的综合性能。



&#x20;   \- **ROC曲线（Receiver Operating Characteristic Curve）和AUC（Area Under the Curve）**

&#x20;       \- \*\*定义\*\*：ROC曲线是以假正率（$FPR = \frac{FP}{TN + FP}$）为横轴，真正率（$TPR = \frac{TP}{TP + FN}$）为纵轴绘制的曲线。AUC是ROC曲线下的面积，其取值范围是0.5到1之间。AUC越大，模型的分类性能越好。

&#x20;       \- \*\*示例\*\*：通过不断调整分类模型的阈值，计算出不同阈值下的FPR和TPR，绘制出ROC曲线。如果AUC为0.9，说明模型有很好的分类性能；如果AUC为0.5，则表示模型的分类效果等同于随机猜测。

&#x20;       \- \*\*适用场景\*\*：在比较不同分类模型的性能或者评估模型在不同阈值下的性能变化时非常有用，尤其是当数据类别不平衡或者关注模型对正负类的区分能力时。



* **回归任务指标**



&#x20;   \- **均方误差（Mean Squared Error，MSE）**

&#x20;       \- \*\*定义\*\*：MSE是预测值与真实值之差的平方的平均值。公式为：$MSE=\frac{1}{n}\sum\_{i = 1}^{n}(y\_{i}-\hat{y}\_{i})^{2}$，其中 $$n$$ 是样本数量，$y\_{i}$ 是第 $$i$$ 个样本的真实值，$\hat{y}\_{i}$ 是第 $$i$$ 个样本的预测值。

&#x20;       \- \*\*示例\*\*：在房价预测任务中，假设有5个房子，真实价格分别为\[100, 120, 150, 90, 110]，模型预测价格为\[90, 130, 140, 80, 120]，则MSE为：$\frac{(100 - 90)^{2}+(120 - 130)^{2}+(150 - 140)^{2}+(90 - 80)^{2}+(110 - 120)^{2}}{5}=100$。

&#x20;       \- \*\*适用场景\*\*：对预测值与真实值的偏差比较敏感，能够很好地衡量模型预测的准确性，在回归任务中广泛使用。但MSE的值会受到数据量纲的影响，并且由于是平方运算，对异常值比较敏感。



&#x20;   \- **均方根误差（Root Mean Squared Error，RMSE）**

&#x20;       \- \*\*定义\*\*：RMSE是MSE的平方根。公式为：$RMSE=\sqrt{\frac{1}{n}\sum\_{i = 1}^{n}(y\_{i}-\hat{y}\_{i})^{2}}$。

&#x20;       \- \*\*示例\*\*：对于上述房价预测的例子，RMSE为$\sqrt{100}=10$。

&#x20;       \- \*\*适用场景\*\*：与MSE类似，但RMSE的量纲与原始数据相同，更便于直观地理解预测误差的大小。



&#x20;   \- **平均绝对误差（Mean Absolute Error，MAE）**

&#x20;       \- \*\*定义\*\*：MAE是预测值与真实值之差的绝对值的平均值。公式为：$MAE=\frac{1}{n}\sum\_{i = 1}^{n}\vert y\_{i}-\hat{y}\_{i}\vert$。

&#x20;       \- \*\*示例\*\*：对于前面房价预测的例子，MAE为：$\frac{\vert100 - 90\vert+\vert120 - 130\vert+\vert150 - 140\vert+\vert90 - 80\vert+\vert110 - 120\vert}{5}=8$。

&#x20;       \- \*\*适用场景\*\*：相比MSE和RMSE，MAE对异常值的鲁棒性更强，因为它是取绝对值而不是平方运算。当数据中存在异常值且不希望异常值对误差指标产生过大影响时，MAE是一个较好的选择。



&#x20;   \- \*\*决定系数（Coefficient of Determination，\*\*\*\*$R^{2}$\*\***）**

&#x20;       \- \*\*定义\*\*：$R^{2}$用于衡量回归模型对数据的拟合程度，其取值范围是从负无穷到1。$R^{2}=1-\frac{\sum\_{i = 1}^{n}(y\_{i}-\hat{y}\_{i})^{2}}{\sum\_{i = 1}^{n}(y\_{i}-\overline{y})^{2}}$，其中 $$\overline{y}$$ 是真实值的平均值。当 $$R^{2}=1$$ 时，表示模型完全拟合数据；当 $$R^{2}=0$$ 时，表示模型预测效果等同于使用均值进行预测；当 $$R^{2}<0$$ 时，表示模型的拟合效果比使用均值预测还差。

&#x20;       \- \*\*示例\*\*：假设在一个简单的线性回归模型中，计算得到 $R^{2}=0.8$，说明模型能够解释 $$80\%$$ 的数据方差，拟合程度较好。

&#x20;       \- \*\*适用场景\*\*：在评估回归模型整体拟合优度时非常有用，能够直观地看出模型对数据变化的解释能力。



## 元学习

Meta learing

少样本学习

Few-shot learning

## SVM

支持向量机 这个为什么考的这么多 既然开始认真学了，考出来就要自信

logistic函数=sigmoid函数

### SVM 优化方法



* 引入核函数：将低维空间中的数据映射到高维空间，使数据在高维空间中变得线性可分，常用的核函数有线性核、多项式核、高斯核等。

* 采用软间隔：允许存在一些样本点违反间隔约束，通过引入松弛变量和惩罚参数，在最大化间隔和最小化分类错误之间进行权衡。

* 使用序列最小优化算法（SMO）：将原问题分解为一系列子问题，每次只优化两个变量，通过不断迭代求解子问题来逼近原问题的最优解，能够高效地求解 SVM 的对偶问题。

## 降维方法

### PCA主成分分析

1. 数据标准化（使数据均值为0   计算协方差矩阵 下述的矩阵操作是对协方差矩阵

2. 计算特征值和特征向量：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量，特征值的大小表示了对应特征向量所包含的信息量的多少。

3. 主成分就是特征值的前几个：按照特征值从大到小的顺序选择前 k 个特征向量，组成投影矩阵，将原始数据投影到这 k 个特征向量所张成的低维空间中，实现降维。

## 机器学习你会什么啊……

贝叶斯分类器也得补吧

# 搜索推荐

什么是你应该会的部分

1. 基于内容的特征推荐

分析物品的特征属性

通过词袋模型、TF-IDF 等方法对物品内容进行特征提取和向量化表示，计算物品之间的相似度，如余弦相似度

## 词袋

统计文本出现次数

## tf\_idf

## 贝叶斯分类

1. 贝叶斯定理&#x20;

有一种 a基于b的条件概率比上a发生的概率=b基于a的条件概率比上b发生的概率  的感觉

![](https://gvl632fo2qu.feishu.cn/space/api/box/stream/download/asynccode/?code=ODE1NDdmNmVlOTUzMmUwNWExMmZjMWFlOGYxYTYyYzVfbFVKdVp2WEdldnp2YnhIeXJ1U2ZDN0ZmYk41VlN0d0VfVG9rZW46QkJRV2J5TGt6b2xUeUt4cWRPa2M1bWNFblZiXzE3MzczNjA3MjI6MTczNzM2NDMyMl9WNA)



将样本分类到后验概率最大的类别中

* 朴素贝叶斯分类器 就是特征之间相互独立

* 半朴素，会适当考虑特征之间的依赖关系 独依赖估计



谷歌 拼写检查依赖贝叶斯方法 补全依赖马尔科夫链 搞笑



# 应用

提示工程、函数调用、RAG还是微调大模型？



模型部署后？

1. 模型更新

   1. 在线学习 更新模型参数：新的模型参数=原模型参数减去学习率乘损失函数在原参数和新数据样本上的梯度 过拟合

   2. 批量学习  同样更新模型参数：多了一个在批量大小中求和

2. 模型优化  对模型精度有影响

   1. 剪枝：模型部署后取出不重要的神经元和权重

      1. 贪婪算法greedy

      2. 删除（去权重的绝对值，神经元的活跃度）选择一个最不重要的神经元或权重

   2. 量化：将模型参数从浮点数转化为整数  实现模型压缩&#x20;

      量化算法主要包括以下几种：

      \- **均匀量化**：将连续的浮点数均匀地映射到整数表示。

      \- **最小 - 最大量化**：将连续的浮点数映射到最小值和最大值之间的整数表示。

      \- **均匀量化和最小 - 最大量化结合**：结合均匀量化和最小 - 最大量化的优点。





# Linux

[其实你Linux不会一点吧](https://www.julyedu.com/questions/interview-list?kp_id=35\&subject=Linux%E7%B3%BB%E7%BB%9F)

```bash
以下是编写简单Linux脚本的步骤：
 
1. 创建脚本文件
 
使用文本编辑器（如 vi 或 nano ）创建一个新文件，文件后缀一般是 .sh ，表示这是一个Shell脚本。例如，创建一个名为 test.sh 的文件。
 
2. 添加脚本头部
 
在脚本的第一行添加 #!/bin/bash ，这一行告诉系统该脚本使用Bash来解释执行。
 
3. 编写命令
 
在头部之后，添加想要执行的Linux命令。例如，要输出“Hello, World!”，可以添加 echo "Hello, World!" 。
 
4. 设置脚本权限
 
保存文件后，需要给脚本添加执行权限。在终端中使用 chmod +x test.sh 命令，其中 test.sh 是你的脚本文件名。
 
5. 执行脚本
 
在终端中，通过 ./test.sh 来运行脚本，就会看到输出的“Hello, World!”。
 
这就是一个简单的Linux脚本编写和运行的过程，你可以根据需要添加更多复杂的命令来完成各种任务。
```



# Python

1. python是什么

Python是解释型语言。这意味着不像C和其他语言，Python运行前不需要编译。其他解释型语言包括PHP和Ruby。



1.Python是动态类型的，这意味着你不需要在声明变量时指定类型。你可以先定义x=111，然后 x=”I’m a string”。

2.Python是面向对象语言，所有允许定义类并且可以继承和组合。Python没有访问访问标识如在C++中的public, private, 这就非常信任程序员的素质，相信每个程序员都是“成人”了\~

3.在Python中，函数是一等公民。这就意味着它们可以被赋值，从其他函数返回值，并且传递函数对象。类不是一等公民。**函数是一等公民**的核心特性是它能像普通数据类型一样被自由传递、返回值、赋值，而类虽然本质上是对象，但它的主要职责是提供模板来创建实例，无法完全实现函数的自由操作。

4.写Python代码很快，但是跑起来会比编译型语言慢。幸运的是，Python允许使用C扩展写程序，所以瓶颈可以得到处理。Numpy库就是一个很好例子，因为很多代码不是Python直接写的，所以运行很快。

5.Python使用场景很多 – web应用开发、大数据应用、数据科学、人工智能等等。它也经常被看做“胶水”语言，使得不同语言间可以衔接上。

6.Python能够简化工作  ，使得程序员能够关心如何重写代码而不是详细看一遍底层实现。

* 浅拷贝 深拷贝

浅拷贝：创建一个新的对象，但它包含的是对原始对象中包含项的引用（如果用引用的方式修改其中一个对象，另外一个也会修改改变）

{1,对列表进行切片（`[:]`），创建一个新列表对象。；2，工厂函数，如list()；3，copy模块的copy()函数}



深拷贝：创建一个新的对象，并且递归的复制它所包含的对象（修改其中一个，另外一个不会改变）{copy模块的deep.deepcopy()函数}

* 全局锁  在CPython中，GIL是一把互斥锁。它的主要作用是保证同一时刻只有一个线程在执行Python字节码。这是因为Python的内存管理不是线程安全的。执行多线程程序的时候效率不高

  1. 在多线程编程中，多个线程可能同时访问和修改共享资源，如果没有适当的同步机制，就会导致数据竞争和不一致的问题，所以需要锁来保证同一时间只有一个线程能够访问共享资源，从而保证数据的完整性和一致性。C++ 中常见的锁包括`std::mutex`（互斥锁

## TensorFlow

* TensorFlow 计算图（Computation Graph）是一种有向图，用于表示计算任务。

  * 图中的节点（Node）代表数学运算操作，比如加法、乘法、卷积等操作，而边（Edge）则代表在这些运算之间流动的数据，这些数据通常是张量（Tensor）。张量可以看作是一个多维数组，例如，一个标量是 0 维张量，一个向量是 1 维张量，一个矩阵是 2 维张量等。

  * 接口op 在计算图中向外界暴露出的接口

* 还有可视化选项

## 算法题

python的字典时间复杂度是1啊，所以经常在计数 就不用遍历了





# 数据

1. 不适合深度学习的数据集

   1. 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

   2. 数据集没有局部相关特性

## 1. 过拟合欠拟合

1. **训练集误差很低**：模型在训练数据上的预测准确率高，损失低。**验证集或测试集误差高**：模型在未见过的数据上预测准确率低，损失高。

2. 模型过于拟合训练数据中的噪声或细节，模型参数或容量过大，而无法有效概括训练数据以外的模式。

3. 没有对模型的自由度进行限制，例如缺乏正则化手段（如 L1/L2 正则化）。

4. 数据处理角度：数据归一化和标准化 降维 数据增强

5. 网络 Bagging

* 定义：Bagging（Bootstrap Aggregating）是一种集成学习方法，用于组合多个基模型来提高模型的性能和稳定性。

- Dropout

* 定义：Dropout 是一种在深度学习模型训练过程中常用的正则化技术。它在训练阶段以一定的概率随机地 “丢弃” 神经元，使得每次迭代时网络的结构都有所不同。



3. 不平衡数据集

   1. 指数据集中不同类别的样本数量存在显著差异的情况。

   2. 欠采样、过采样和生成合成数据：这三种方法通常在训练分类器之前使用以平衡数据集。简单来说：欠采样：从样本较多的类中再抽取，仅保留这些样本点的一部分；过采样：复制少数类中的一些点，以增加其基数；生成合成数据：从少数类创建新的合成点，以增加其基数。

4. 首先是维度灾难，36个指标意味着更高的数据复杂性和计算成本。更多的指标可能会引入噪声，这些噪声指标可能会掩盖数据真实的结构和规律。例如，在一个包含大量无关变量的回归模型中，可能会导致过拟合，模型在训练数据上表现很好，但在新数据上效果很差。
   \- 提及实际影响：从实际应用角度，36个指标可能会使模型训练和预测速度变慢，尤其在处理大规模数据或者对实时性要求较高的场景下，不筛选指标会带来很多不便。并且如果这些指标间存在多重共线性，会影响模型的稳定性和可解释性。

## 2. 批归一化BN

用于对神经网络每层的输入数据进行归一化处理，从而加速训练过程并且能够在一定程度上提高模型的泛化能力。

* 数据蒸馏

是一种机器学习技术，主要用于减少训练数据集的大小，同时尽量保持[模型性能](https://edu.csdn.net/cloud/pm_summit?utm_source=blogglc\&spm=1001.2101.3001.7020)不变。这种方法特别适用于那些拥有大量标记数据的情况，其中一部分数据可能是冗余的或不那么重要的。数据蒸馏可以帮助提高模型训练的速度和效率，同时也降低了存储和计算资源的需求。

数据蒸馏的基本思想是从原始的大规模数据集中提取出一个较小的数据子集，该子集能以较小的代价维持原始模型的性能。通常的做法是利用一个已经训练好的模型（教师模型）来生成一个新的小规模数据集，然后使用这个数据集来训练另一个模型（学生模型），以达到与原始模型相似的性能。

## 数据挖掘

1. 单变量分析

   1. 数值型&#x20;

   2. 类别型 频次频率

2. 多变量

   1. 皮尔逊相关系数 斯皮尔曼等级相关系数

   2. 主成分降维

3. 领域知识的特征挖掘

   1. 怎么把网球编一编

## 数据处理

自信点 你真的都知道都用过 但面试的时候得全面一点啊

1. 数据标准

   1. z-score标准化 将数据转化为均值为0 标准差为1

2. 归一化

   1. 更多适用于数据的量纲差异较大且数据分布不明确的场景，例如在神经网络中，对输入数据进行归一化有助于梯度下降算法更好地收敛，避免某些特征因为数值过大而主导模型训练

3. 数据清洗

   1. 数据缺失值&#x20;

      1. 对于数值型数据，可以采用均值、中位数或众数填充；

      2. 对于类别型数据，可以使用众数填充或创建一个新的类别表示缺失。

   2. 噪声

      1. 可以通过统计方法（如计算均值和标准差，将超出一定范围的值视为异常）或基于数据分布（如箱线图）来识别并处理，

## 数据增强

* 翻转（水平翻转、垂直翻转）、旋转、缩放、裁剪、平移、

* 添加噪声（如高斯噪声、椒盐噪声）、

* 颜色变换（亮度调整、对比度调整、饱和度调整等）。

能更好的应对真实情况

## batch Patch Layer

1. Batch normalization

2. Layer n



* batch是批次，是数据集的子集

被问住过的一个问题  什么是batch什么是批次  其实只是单纯的分块吧

假设我们有一个包含 1000 张图像的图像分类数据集，每张图像的大小为 32x32 像素，并且是 RGB 图像（即有 3 个通道）

那么 对于每个 mini-batch，`batch_data` 的形状是 `(32, 32, 32, 3)`，表示这个 mini-batch 包含 32 个样本，每个样本是 32x32 的 RGB 图像；`batch_labels` 的形状是 `(32,)`，表示这 32 个样本对应的标签。

* patch是小块，是把单个样本分割

图像数据和序列数据相关 将输入数据划分为小块  对于单个样本分解为局部特征表示



* Batch：操作对象是整个数据集，维度一般为 `(batch_size,...)`，其中 `...` 表示样本的特征维度，如 `(batch_size, channels, height, width)` 表示图像数据。

* Patch：操作对象是单个样本，对于图像，维度通常从 `(channels, height, width)` 转换为 `(num_patches, patch_size)` 或 `(num_patches, channels, patch_height, patch_width)`，对于序列，从 `(sequence_length,...)` 转换为 `(num_patches, patch_length,...)`。

## 正则化

* l\_1正则化：正则化系数\*模型权重的一范数

很多权重会变成零，在零点不可导，会使模型的权重产生稀疏性，即很多权重会变为 0，从而实现特征选择的功能。

* l\_2正则化：正则化系数\*模型权重的二范数

会使模型的权重趋于较小的值，但不会使权重变为 0，主要用于防止模型过拟合，使模型更加稳定

# 数据结构

### 稀疏矩阵



是指矩阵中大部分元素为 0 的矩阵。与稠密矩阵相比，稀疏矩阵可以用更高效的方式存储和处理，例如采用压缩存储格式，如三元组表、十字链表等

# 算法题

[ 刷题](https://gvl632fo2qu.feishu.cn/docx/UBmDdlFVqoxMg6xt1bWcog2SnJg?from=from_copylink)

先别害怕 你超级聪明

也就是经常问一问target数组

## 两数之和

```python
def two_sum(nums, target):
    num_dict = {}
    for index, num in enumerate(nums):
        complement = target - num
        if complement in num_dict:
            return [num_dict[complement], index]
        num_dict[num] = index
    return []

# 示例用法
nums = [2, 7, 11, 15]
target = 9
result = two_sum(nums, target)
print(result)
```

怎么返回index
